{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import struct\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from six.moves import cPickle\n",
    "from utils import *\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 20 #latent dimension\n",
    "n_hidden = 500 # hidden units\n",
    "lr = 0.001 # learning rate\n",
    "beta_reg = 1.0 # for beta-VAE, 1.0 corresponds to usual VAE\n",
    "gamma = 0.0 # warping parameter\n",
    "N = 60000 # number of train samples in MNIST\n",
    "N_test = 10000 # number of test samples in MNIST\n",
    "batch_size = 50\n",
    "max_epochs = 1 # number of epochs used for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ADDED Hyperparam overrides\n",
    "max_epochs = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VAE IMPLEMENTATION\n",
    "tf.reset_default_graph()\n",
    "\n",
    "eps = tf.placeholder(tf.float32, shape=(None, d))  # variables to reparametrize to sample from approx posterior\n",
    "z_gen = tf.placeholder(tf.float32, shape=(None, d))  # used only for generative samples (samples from prior)\n",
    "X = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "labels = tf.placeholder(tf.float32, shape=(None, 10))\n",
    "# the following two placeholders should be obtained by running the graph and generating samples from the model\n",
    "# they are used to compute the inception score\n",
    "class_distr_gen = tf.placeholder(tf.float32, shape=(10))  # this is intended to be the generative label distribution\n",
    "X_gen = tf.placeholder(tf.float32, shape=(None, 784))  # these are generated samples\n",
    "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
    "\n",
    "# the following lines are only used to have tensorflow versions of the mean and inverse mean functions\n",
    "input_vals = tf.placeholder(tf.float32, shape=(None, 784))\n",
    "input_means = tf.clip_by_value(cont_bern_mean(input_vals), 1e-4, 1-1e-4)\n",
    "input_lams = tf.clip_by_value(cont_bern_lam(input_vals), 1e-4, 1-1e-4)\n",
    "\n",
    "with tf.variable_scope('ELBO', reuse=tf.AUTO_REUSE):\n",
    "    mu_Z, sigma_Z = encoder_mnist(X, n_hidden, d, keep_prob)\n",
    "Z = mu_Z + sigma_Z * eps  # [batch_size, d]\n",
    "\n",
    "with tf.variable_scope('ELBO', reuse=tf.AUTO_REUSE):\n",
    "    lam = decoder_mnist_cb(Z, n_hidden, 784, keep_prob)\n",
    "    lam = tf.clip_by_value(lam, 1e-4, 1 - 1e-4)\n",
    "    # sample from the model\n",
    "    lam_gen = decoder_mnist_cb(z_gen, n_hidden, 784, keep_prob)\n",
    "    lam_gen = tf.clip_by_value(lam_gen, 1e-4, 1 - 1e-4)\n",
    "\n",
    "log_norm_const = cont_bern_log_norm(lam)\n",
    "log_p_all = tf.reduce_sum(X * tf.log(lam) + (1 - X) * tf.log(1 - lam) + log_norm_const, 1)\n",
    "log_p = tf.reduce_mean(log_p_all)\n",
    "\n",
    "# computing an IW estimate of the log likelihood requires having k epsilon samples per data point in the batch,\n",
    "# so that eps would have to be shaped [None, d, k], which would complicate the rest of the graph.\n",
    "# since trainin with IWAE is not required here, only the log importance weights are computed in the graph and the\n",
    "# log likelihood estimate is computed outside the graph by calling it several times for the same batch but with\n",
    "# different random epsilons.\n",
    "log_iw = log_p_all + tf.reduce_sum(-0.5 * tf.square(Z), 1)\n",
    "log_iw = log_iw + tf.reduce_sum(tf.log(1e-8 + sigma_Z) + tf.square(mu_Z - Z) / (2.0 * tf.square(sigma_Z)), 1)\n",
    "\n",
    "KL = 0.5 * tf.reduce_sum(tf.square(mu_Z) + tf.square(sigma_Z) - tf.log(1e-8 + tf.square(sigma_Z)) - 1.0, 1)\n",
    "KL = tf.reduce_mean(KL)\n",
    "\n",
    "ELBO = log_p - beta_reg * KL\n",
    "cost = - ELBO\n",
    "\n",
    "log_p_all_cheat = tf.reduce_sum(X * tf.log(lam) + (1 - X) * tf.log(1 - lam), 1)\n",
    "log_p_cheat = tf.reduce_mean(log_p_all_cheat)\n",
    "ELBO_cheat = log_p_cheat - beta_reg * KL\n",
    "cost_cheat = - ELBO_cheat\n",
    "log_iw_cheat = log_iw - log_p_all + log_p_all_cheat\n",
    "\n",
    "mean_from_lam = cont_bern_lam(lam)\n",
    "mean_from_lam = tf.clip_by_value(mean_from_lam, 1e-4, 1 - 1e-4)\n",
    "\n",
    "log_norm_const_from_mean = cont_bern_log_norm(mean_from_lam)\n",
    "\n",
    "log_p_all_from_mean = tf.reduce_sum(X * tf.log(mean_from_lam) + (1 - X) * tf.log(1 - mean_from_lam) +\n",
    "                                    log_norm_const_from_mean, 1)\n",
    "log_p_from_mean = tf.reduce_mean(log_p_all_from_mean)\n",
    "ELBO_from_mean = log_p_from_mean - beta_reg *  KL\n",
    "log_iw_from_mean = log_iw - log_p_all + log_p_all_from_mean\n",
    "\n",
    "log_p_all_from_mean_cheat = tf.reduce_sum(X * tf.log(mean_from_lam) + (1 - X) * tf.log(1 - mean_from_lam), 1)\n",
    "log_p_from_mean_cheat = tf.reduce_mean(log_p_all_from_mean_cheat)\n",
    "ELBO_from_mean_cheat = log_p_from_mean - KL\n",
    "log_iw_from_mean_cheat = log_iw - log_p_all + log_p_all_from_mean_cheat\n",
    "\n",
    "with tf.variable_scope('classifier', reuse=tf.AUTO_REUSE):\n",
    "    class_logits = classifier_mnist(X, n_hidden, 10, keep_prob)\n",
    "    class_logits_gen = classifier_mnist(X_gen, n_hidden, 10, keep_prob)\n",
    "acc = 1.0 - tf.reduce_mean(tf.abs(tf.sign(tf.cast(tf.argmax(class_logits, 1) - tf.argmax(labels, 1), tf.float32))))\n",
    "class_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=class_logits, labels=labels))\n",
    "class_probs_gen = tf.clip_by_value(tf.nn.softmax(class_logits_gen), 1e-4, 1 - 1e-4)\n",
    "is_kl = tf.reduce_sum(class_probs_gen * (tf.log(class_probs_gen) - tf.log(class_distr_gen)), axis=1)\n",
    "log_is = tf.reduce_mean(is_kl)\n",
    "\n",
    "optim = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "all_params_ELBO = tf.trainable_variables(scope='ELBO')\n",
    "grads_and_vars = optim.compute_gradients(cost, all_params_ELBO)\n",
    "clipped_grads_and_vars = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in grads_and_vars]\n",
    "optimizer = optim.apply_gradients(clipped_grads_and_vars)\n",
    "\n",
    "optim_cheat = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "grads_and_vars_cheat = optim_cheat.compute_gradients(cost_cheat, all_params_ELBO)\n",
    "clipped_grads_and_vars_cheat = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in grads_and_vars_cheat]\n",
    "optimizer_cheat = optim_cheat.apply_gradients(clipped_grads_and_vars_cheat)\n",
    "\n",
    "optim_class = tf.train.AdamOptimizer(learning_rate=lr)\n",
    "all_params_class = tf.trainable_variables(scope='classifier')\n",
    "grads_and_vars_class = optim.compute_gradients(class_loss, all_params_class)\n",
    "clipped_grads_and_vars_class = [(tf.clip_by_value(grad, -1.0, 1.0), var) for grad, var in grads_and_vars_class]\n",
    "optimizer_class = optim_class.apply_gradients(clipped_grads_and_vars_class)\n",
    "\n",
    "init_op = tf.global_variables_initializer()\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPER FUNCTIONS TO EVALUATE METRICS ONCE THE MODEL IS TRAINED\n",
    "\n",
    "def epoch_metrics_cb(N, d, batch_size, X_eval, k=100):\n",
    "    # like epoch_metrics, but also computes the ELBO and log lik estimates after applying the continuous Bernoulli\n",
    "    # inverse mean function to the output of the decoder\n",
    "    assert np.shape(X_eval)[0] % batch_size == 0\n",
    "    ELBO_val = 0.0\n",
    "    ELBO_cheat_val = 0.0\n",
    "    IW_ELBO = 0.0\n",
    "    IW_cheat = 0.0\n",
    "    ELBO_fm_val = 0.0\n",
    "    ELBO_fm_cheat_val = 0.0\n",
    "    IW_fm_ELBO = 0.0\n",
    "    IW_fm_cheat = 0.0\n",
    "    local_perms = PermManager(np.shape(X_eval)[0], batch_size)\n",
    "    while local_perms.epoch < 1:\n",
    "        eps_batch = np.random.normal(size=[batch_size, d])\n",
    "        batch = local_perms.get_indices()\n",
    "        X_batch = np.reshape(X_eval[batch, :], [batch_size, -1])\n",
    "        ELBO_val_batch, ELBO_cheat_val_batch, ELBO_fm_val_batch, ELBO_fm_cheat_val_batch = sess.run([ELBO, ELBO_cheat,\n",
    "                                                                                                     ELBO_from_mean,\n",
    "                                                                                                     ELBO_from_mean_cheat],\n",
    "                                                                                                    {eps: eps_batch,\n",
    "                                                                                                     X: X_batch,\n",
    "                                                                                                     keep_prob: 1.0})\n",
    "        ELBO_val += ELBO_val_batch\n",
    "        ELBO_cheat_val += ELBO_cheat_val_batch\n",
    "        ELBO_fm_val += ELBO_fm_val_batch\n",
    "        ELBO_fm_cheat_val += ELBO_fm_cheat_val_batch\n",
    "        increase_IW_ELBO = []\n",
    "        increase_IW_cheat = []\n",
    "        increase_IW_fm_ELBO = []\n",
    "        increase_IW_fm_cheat = []\n",
    "        for i in xrange(k):\n",
    "            eps_batch = np.random.normal(size=[batch_size, d])\n",
    "            inc_IW_ELBO, inc_IW_cheat, inc_IW_fm_ELBO, inc_IW_fm_cheat = sess.run([log_iw, log_iw_cheat,\n",
    "                                                                                    log_iw_from_mean,\n",
    "                                                                                    log_iw_from_mean_cheat],\n",
    "                                                                                   {eps: eps_batch, X: X_batch,\n",
    "                                                                                    keep_prob: 1.0})\n",
    "            increase_IW_ELBO.append(inc_IW_ELBO)\n",
    "            increase_IW_cheat.append(inc_IW_cheat)\n",
    "            increase_IW_fm_ELBO.append(inc_IW_fm_ELBO)\n",
    "            increase_IW_fm_cheat.append(inc_IW_fm_cheat)\n",
    "        increase_IW_ELBO = np.array(increase_IW_ELBO)\n",
    "        a_ELBO = np.max(increase_IW_ELBO, axis=0)\n",
    "        IW_ELBO += np.mean(a_ELBO + np.log(np.sum(np.exp(increase_IW_ELBO - a_ELBO), 0)) - np.log(k))\n",
    "        increase_IW_cheat = np.array(increase_IW_cheat)\n",
    "        a_cheat = np.max(increase_IW_cheat, axis=0)\n",
    "        IW_cheat += np.mean(a_cheat + np.log(np.sum(np.exp(increase_IW_cheat - a_cheat), 0)) - np.log(k))\n",
    "        increase_IW_fm_ELBO = np.array(increase_IW_fm_ELBO)\n",
    "        a_fm_ELBO = np.max(increase_IW_fm_ELBO, axis=0)\n",
    "        IW_fm_ELBO += np.mean(a_fm_ELBO + np.log(np.sum(np.exp(increase_IW_fm_ELBO - a_fm_ELBO), 0)) - np.log(k))\n",
    "        increase_IW_fm_cheat = np.array(increase_IW_fm_cheat)\n",
    "        a_fm_cheat = np.max(increase_IW_fm_cheat, axis=0)\n",
    "        IW_fm_cheat += np.mean(a_fm_cheat + np.log(np.sum(np.exp(increase_IW_fm_cheat - a_fm_cheat), 0)) - np.log(k))\n",
    "    ELBO_val = ELBO_val * batch_size / N\n",
    "    ELBO_cheat_val = ELBO_cheat_val * batch_size / N\n",
    "    IW_ELBO = IW_ELBO * batch_size / N\n",
    "    IW_cheat = IW_cheat * batch_size / N\n",
    "    ELBO_fm_val = ELBO_fm_val * batch_size / N\n",
    "    ELBO_fm_cheat_val = ELBO_fm_cheat_val * batch_size / N\n",
    "    IW_fm_ELBO = IW_fm_ELBO * batch_size / N\n",
    "    IW_fm_cheat = IW_fm_cheat * batch_size / N\n",
    "    del local_perms\n",
    "    return ELBO_val, ELBO_cheat_val, IW_ELBO, IW_cheat, ELBO_fm_val, ELBO_fm_cheat_val, IW_fm_ELBO, IW_fm_cheat\n",
    "\n",
    "\n",
    "def compute_IS(batch_size, samples):\n",
    "    # computes the inception score using samples\n",
    "    N_samples = np.shape(samples)[0]\n",
    "    class_distr_val = np.zeros(10)\n",
    "    local_perms = PermManager(N_samples, batch_size)\n",
    "    while local_perms.epoch < 1:\n",
    "        batch = local_perms.get_indices()\n",
    "        samples_batch = np.reshape(samples[batch], [batch_size, -1])\n",
    "        class_distr_val += np.sum(sess.run(class_probs_gen, {X_gen: samples_batch, keep_prob: 1.0}), axis=0)\n",
    "    class_distr_val = class_distr_val / N_samples\n",
    "    log_is_val = 0.0\n",
    "    local_perms = PermManager(N_samples, batch_size)\n",
    "    while local_perms.epoch < 1:\n",
    "        batch = local_perms.get_indices()\n",
    "        samples_batch = np.reshape(samples[batch], [batch_size, -1])\n",
    "        log_is_val += sess.run(log_is, {X_gen: samples_batch, class_distr_gen: class_distr_val, keep_prob:1.0})\n",
    "    log_is_val = log_is_val * batch_size / N\n",
    "    is_val = np.exp(log_is_val)\n",
    "    return is_val\n",
    "\n",
    "\n",
    "def mean_from_lam2(lam_vals):\n",
    "    # tensorflow version of mean function\n",
    "    return sess.run(input_means, {input_vals: lam_vals})\n",
    "\n",
    "\n",
    "def lam_from_means2(batch_size, mean_vals):\n",
    "    # tensorflow version of inverse mean function.\n",
    "    # batches over samples\n",
    "    N_loc = np.shape(mean_vals)[0]\n",
    "    assert N_loc % batch_size == 0\n",
    "    lams = []\n",
    "    local_perms = PermManager(N_loc, batch_size, perm=np.arange(N_loc))\n",
    "    while local_perms.epoch < 1:\n",
    "        batch = local_perms.get_indices()\n",
    "        lams.append(sess.run(input_lams, {input_vals: mean_vals[batch]}))\n",
    "    return np.concatenate(lams)\n",
    "\n",
    "\n",
    "def sample_from_model_cb(batch_size, N):\n",
    "    # gives back N samples (i.e. outputs of decoder) from the model\n",
    "    assert N % batch_size == 0\n",
    "    K = N / batch_size\n",
    "    samples = []\n",
    "    for i in xrange(K):\n",
    "        samples.append(sess.run(lam_gen, {z_gen: np.random.normal(size=[batch_size, d]), keep_prob: 1.0}))\n",
    "    return np.concatenate(samples)\n",
    "\n",
    "\n",
    "def k_nn_acc(k, batch_size, X_eval, digits_eval, X_train, digits_train):\n",
    "    # computes the knn metric from the model's latent variables\n",
    "    assert np.shape(X_eval)[0] % batch_size == 0\n",
    "    assert np.shape(X_eval)[0] == np.shape(digits_eval)[0]\n",
    "    assert np.shape(X_train)[0] % batch_size == 0\n",
    "    assert np.shape(X_train)[0] == np.shape(digits_train)[0]\n",
    "    local_perms = PermManager(np.shape(X_train)[0], batch_size, perm=np.arange(np.shape(X_train)[0]))\n",
    "    mu_vals_train = []\n",
    "    while local_perms.epoch < 1:\n",
    "        batch = local_perms.get_indices()\n",
    "        X_batch = np.reshape(X_train[batch, :], [batch_size, -1])\n",
    "        mu_vals_train.append(sess.run(mu_Z, {X: X_batch, keep_prob: 1.0}))\n",
    "    mu_vals_train = np.concatenate(mu_vals_train)\n",
    "    classifier = KNeighborsClassifier(n_neighbors=k)\n",
    "    classifier.fit(mu_vals_train, digits_train)\n",
    "    local_perms = PermManager(np.shape(X_eval)[0], batch_size, perm=np.arange(np.shape(X_eval)[0]))\n",
    "    mu_vals_eval = []\n",
    "    while local_perms.epoch < 1:\n",
    "        batch = local_perms.get_indices()\n",
    "        X_batch = np.reshape(X_eval[batch, :], [batch_size, -1])\n",
    "        mu_vals_eval.append(sess.run(mu_Z, {X: X_batch, keep_prob: 1.0}))\n",
    "    mu_vals_eval = np.concatenate(mu_vals_eval)\n",
    "    digit_pred = classifier.predict(mu_vals_eval)\n",
    "    del local_perms\n",
    "    return float(np.sum(digit_pred == digits_eval)) / np.shape(X_eval)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD AND WARP MNIST\n",
    "train_img, train_digits = read_mnist(path='./mnist/')\n",
    "test_img, test_digits = read_mnist(dataset='testing', path='./mnist/')\n",
    "train_img = (np.array(train_img, dtype='float32') + np.array(np.random.random((60000, 28, 28)),\n",
    "                                                             dtype='float32')) / 256.0\n",
    "train_img = warp(train_img, gamma)\n",
    "test_img = (np.array(test_img, dtype='float32') + np.array(np.random.random((10000, 28, 28)),\n",
    "                                                           dtype='float32')) / 256.0\n",
    "test_img = warp(test_img, gamma)\n",
    "train_lbl = make_one_hot(train_digits, 10)\n",
    "test_lbl = make_one_hot(test_digits, 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CLASSIFIER (USED TO COMPUTE INCEPTION SCORES)\n",
    "\n",
    "# sess.run(init_op)\n",
    "\n",
    "perms = PermManager(N, batch_size)\n",
    "while True:\n",
    "    start_epoch = perms.epoch\n",
    "    batch = perms.get_indices()\n",
    "    X_batch = np.reshape(train_img[batch, :], [batch_size, -1])\n",
    "    labels_batch = train_lbl[batch]\n",
    "    _, c = sess.run([optimizer_class, class_loss], {X: X_batch, labels: labels_batch, keep_prob: 0.9})\n",
    "    if perms.epoch >= max_epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN CONTINUOUS BERNOULLI VAE\n",
    "\n",
    "sess.run(init_op)  # uncomment to restart variables\n",
    "\n",
    "perms = PermManager(N, batch_size)\n",
    "while True:\n",
    "    start_epoch = perms.epoch\n",
    "    eps_batch = np.random.normal(size=[batch_size, d])\n",
    "    batch = perms.get_indices()\n",
    "    X_batch = np.reshape(train_img[batch, :], [batch_size, -1])\n",
    "    _, c = sess.run([optimizer, cost], {eps: eps_batch, X: X_batch, keep_prob: 0.9})\n",
    "    if perms.epoch >= max_epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch metrics computes ELBOs and log likelihoods, both including and ignoring normalizing constants, as well as\n",
    "# transforming the decoder output with the inverse mean function\n",
    "print epoch_metrics_cb(N_test, d, batch_size, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the knn metric to measure usefulness of latents\n",
    "print k_nn_acc(15, batch_size, test_img, test_digits, train_img, train_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 288x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# SHOW MODEL SAMPLES\n",
    "\n",
    "ind = 0\n",
    "samples = sample_from_model_cb(100, 100)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(np.reshape(samples[ind], [28, 28]), norm=None, vmin=0.0, vmax=1.0, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    plt.subplot(240 + i+1)\n",
    "    plt.imshow(np.reshape(samples[i], [28, 28]), norm=None, vmin=0.0, vmax=1.0, cmap='gray')\n",
    "    plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# COMPUTE INCEPTION SCORES\n",
    "\n",
    "samples = sample_from_model_cb(batch_size, N)\n",
    "# uncomment to select which IS to compute\n",
    "# new_samples = np.reshape(train_img, [N, 784]) # data\n",
    "# new_samples = sample_cont_bern(samples) # CB from decoder output\n",
    "# new_samples = sample_cont_bern(mean_from_lam2(samples)) # CB from mu(decoder output)\n",
    "new_samples = samples # decoder output\n",
    "# new_samples = mean_from_lam2(samples) # mu(decoder output)\n",
    "# new_samples = sample_bern(samples) # B from decoder output\n",
    "# new_samples = sample_bern(mean_from_lam2(samples)) # B from mu(decoder output)\n",
    "# new_samples = sample_cont_bern(lam_from_means2(batch_size, samples)) # CB from mu^{-1}(decoder output)\n",
    "# new_samples = lam_from_means2(batch_size, samples) # mu^{-1}(decoder output)\n",
    "# new_samples = sample_bern(lam_from_means2(batch_size, samples)) # B from mu^{-1}(decoder output)\n",
    "\n",
    "print compute_IS(batch_size, new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN BERNOULLI VAE\n",
    "\n",
    "sess.run(init_op)  # uncomment to restart variables, should be done if CB VAE was previously trained\n",
    "\n",
    "perms = PermManager(N, batch_size)\n",
    "while True:\n",
    "    start_epoch = perms.epoch\n",
    "    eps_batch = np.random.normal(size=[batch_size, d])\n",
    "    batch = perms.get_indices()\n",
    "    X_batch = np.reshape(train_img[batch, :], [batch_size, -1])\n",
    "    _, c = sess.run([optimizer_cheat, cost_cheat], {eps: eps_batch, X: X_batch, keep_prob: 0.9})\n",
    "    if perms.epoch >= max_epochs:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SAME AS SOME CELLS ABOVE, MEANT TO BE RUN AFTER HAVING TRAINED THE BERNOULLI VAE FOR COMPARISON\n",
    "print epoch_metrics_cb(N_test, d, batch_size, test_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print k_nn_acc(15, batch_size, test_img, test_digits, train_img, train_digits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind = 0\n",
    "samples = sample_from_model_cb(100, 100)\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "plt.imshow(np.reshape(samples[ind], [28, 28]), norm=None, vmin=0.0, vmax=1.0, cmap='gray')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE THAT CLASSIFIER SHOULD BE RETRAINED BEFORE RUNNING THIS IF THE BERNOULLI VAE WAS THE LAST TRAINED ONE\n",
    "\n",
    "samples = sample_from_model_cb(batch_size, N)\n",
    "# uncomment to select which IS to compute\n",
    "# new_samples = np.reshape(train_img, [N, 784]) # data\n",
    "# new_samples = sample_cont_bern(samples) # CB from decoder output\n",
    "# new_samples = sample_cont_bern(mean_from_lam2(samples)) # CB from mu(decoder output)\n",
    "new_samples = samples # decoder output\n",
    "# new_samples = mean_from_lam2(samples) # mu(decoder output)\n",
    "# new_samples = sample_bern(samples) # B from decoder output\n",
    "# new_samples = sample_bern(mean_from_lam2(samples)) # B from mu(decoder output)\n",
    "# new_samples = sample_cont_bern(lam_from_means2(batch_size, samples)) # CB from mu^{-1}(decoder output)\n",
    "# new_samples = lam_from_means2(batch_size, samples) # mu^{-1}(decoder output)\n",
    "# new_samples = sample_bern(lam_from_means2(batch_size, samples)) # B from mu^{-1}(decoder output)\n",
    "\n",
    "print compute_IS(batch_size, new_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
